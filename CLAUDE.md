# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a research framework for long video understanding called **Video-Agent** (视频世界 API / Video World API). The project takes a novel approach by treating videos not as sequences of frames fed directly to MLLMs, but as interactive environments accessible through a structured API.

### Core Concept

- LLM does not directly process pixels
- Videos are processed by vision models into a queryable "video world"
- LLM interacts with this world through **atomic video operations (world API)** and **high-level tools automatically induced by LLM**
- Strictly **train-free**: Base LLM (e.g., GPT-4o) remains frozen with no task-specific fine-tuning

### Two-Phase Architecture

**Phase 1 - Tool Induction (Training-free Tool Induction)**
- Input: World API specification + task domain description (question types, examples)
- Output: High-level tool definitions (JSON/DSL) generated by LLM
- Tools are compiled into executable functions with OpenAI `tools` schema

**Phase 2 - Tool-based Video Reasoning**
- Uses frozen tool library (atomic + high-level tools)
- Controller LLM interacts with video world and memory through `tools`
- Implements long video QA/reasoning tasks

## System Architecture

### Three-Layer Structure

1. **World Layer (Video World + World API)**
   - Scene segmentation, keyframe/clip sampling
   - Video features, detection/tracking, ASR
   - Embedding indexing
   - Exposes unified atomic world operations

2. **Tool Layer**
   - Atomic tools: Direct mapping to world API
   - High-level tools: LLM-induced workflow compositions of atomic tools

3. **Controller Layer**
   - Accepts questions and current memory state
   - Multi-step function calling over tool list (atomic + high-level)
   - Drives the reasoning process

## World API: Atomic Operations (16 Core Operations)

### 1. Temporal & Scene Navigation
- `list_scenes`: Returns coarse scene segmentation with brief descriptions
- `get_segment`: Converts time range to segment ID with metadata
- `search_segments_by_text`: Retrieves candidate segments by text query

### 2. Local Perception
- `describe_segment`: Natural language description of time segment
- `detect_objects`: Detects objects at specific timestamp/segment
- `recognize_actions`: Identifies main actions/events in time range

### 3. Entity & Trajectory
- `register_entity`: Assigns global `entity_id` to objects of interest
- `get_entity_trajectory`: Queries entity appearance times and trajectories
- `query_entity_state`: Queries entity state/attributes at specific time

### 4. Semantic Retrieval
- `search_segments_by_semantics`: Semantic similarity search within time range
- `search_similar_to_example`: Finds visually/semantically similar segments

### 5. Hierarchical Memory Operations
- `write_memory`: Writes summaries to memory pool (frame/segment/event level)
- `read_memory`: Retrieves relevant memory entries by query
- `merge_events`: Merges multiple event-level memories into higher-level summaries

### 6. Meta Operations
- `list_entities`: Lists currently registered entities
- `get_video_metadata`: Returns basic video metadata (duration, fps, resolution)

## API Design Principles

1. **Action-oriented, not model-oriented**: Describes "what can be done" not implementation details
2. **Unified spatiotemporal and entity representation**: Common parameters across operations
   - `video_id`, `timestamp`, `time_range {start_time, end_time}`
   - `entity_id`, `entity_hint`, `text_query`
   - `level ∈ {frame, segment, event}`
3. **Moderate atomic granularity**: Suitable as building blocks for high-level tools
4. **LLM-friendly interface**: Clear names, 2-6 parameters, type information
5. **Stateless vs Stateful distinction**: Query-only vs memory-modifying operations

## High-Level Tool Definition Format

Tools are defined in JSON with executable steps:

```json
{
  "tool_name": "locate_first_appearance",
  "description": "根据人物描述,定位该人物在视频中首次出现的时间段",
  "inputs": {
    "video_id": "string",
    "person_query": "string"
  },
  "outputs": {
    "start_time": "number",
    "end_time": "number",
    "evidence": "string"
  },
  "steps": [
    {
      "type": "call_world_api",
      "api": "search_segments_by_text",
      "params": {
        "video_id": "{{video_id}}",
        "query": "{{person_query}}",
        "top_k": 20
      },
      "save_as": "candidates"
    }
  ]
}
```

## Implementation Milestones

**M1 - World API Prototype**
- Select long video dataset or synthetic sandbox
- Implement core atomic operations (list_scenes, describe_segment, search_segments_by_text, etc.)

**M2 - Simplified Tool Induction**
- Run Phase 1 on single task domain A to generate `ToolLib_A`
- Manual validation of generated tools

**M3 - Complete Tool Library + Phase 2 Reasoning**
- Build typical high-level tools
- Implement Controller + memory system
- Complete Q→tool-calls→answer pipeline on small-scale data

**M4 - Multi-domain + OOD Experiments**
- Add task domains B, C with corresponding tool libraries
- In-domain and cross-domain comparisons

**M5 - Paper Writing & Visualization**
- World API specification tables, tool examples, case studies

## Research Questions

1. How to design LLM-friendly, extensible atomic video operation library?
2. How to automatically induce high-level tool library in train-free manner?
3. What advantages does hierarchical tooling provide over:
   - Atomic tools only
   - Per-question pipelines
   - End-to-end MLLMs
   In terms of performance, efficiency, and OOD task domain adaptation?

## Task Domain Classification

Tasks are organized by type:
- **Domain A**: Localization + Counting
- **Domain B**: Sequential + Causal reasoning
- **Domain C**: Summarization / Multi-segment reasoning

## Language Note

Project documentation is primarily in Chinese (Simplified). Core concepts use bilingual terminology (Chinese + English).
